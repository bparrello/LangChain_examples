{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IJvl-EEESCA"
      },
      "source": [
        "# <font color=red>LangChain:  Memory</font>\n",
        "- https://docs.langchain.com/docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What Does LangChain Provide?\n",
        "+ Models\n",
        "  + embedding\n",
        "  + LLM (e.g. OpenAI)\n",
        "+ Prompts\n",
        "  + prompt templates\n",
        "  + few-shot\n",
        "  + example-selectors\n",
        "  + output parsers\n",
        "+ Chains (a multi-step workflow composed of <em>links</em>)</br>\n",
        "  + Links (one of: prompt, model, another chain)\n",
        "+ Vector Database Access\n",
        "  + Document Loaders\n",
        "  + Text Splitting \n",
        "<span style=\"font-family:'Comic Sans MS', cursive, sans-serif;\"><font color=orange>\n",
        "+ Memories (to facilitate chatbots or other 'iterative' sorts of apps)\n",
        "</font></span>\n",
        "+ Agents (loop over Thought, Act, Observe)\n",
        "  + Tools\n",
        "    + math\n",
        "    + web search\n",
        "    + custom (user-defined)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"font-family:'Comic Sans MS', cursive, sans-serif;\"><font color=orange>\n",
        "## Memories\n",
        "</font></span>\n",
        "There are several types of memory available with LangChain.</br>\n",
        "We will examine a simple version in a simple application, and then look at a couple of</br>\n",
        "examples that use memory in concert with chains and/or agents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, let's do it <em>withOUT</em> memory.  We will use a plain LLMChain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0, max_tokens=64)\n",
        "\n",
        "template = \"\"\"\n",
        "You are a nice chatbot having a conversation with a human.\n",
        "New human question: {question}\n",
        "Response:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "conversation = LLMChain(llm=llm, prompt=prompt, verbose=False)\n",
        "\n",
        "response = conversation({\"question\": \"hi, my name is Gizmo\"})\n",
        "print(response[\"text\"])\n",
        "\n",
        "response = conversation({\"question\": \"what is the name of the first USA president?\"})\n",
        "print(response[\"text\"])\n",
        "\n",
        "## NOTE: we expect that the llm will NOT remember us\n",
        "response = conversation({\"question\": \"do you recall my name?\"})\n",
        "print(response[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, let's do it <em>with</em> memory.  We will continue to use a plain LLMChain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0, max_tokens=64)\n",
        "\n",
        "# NOTE: \"chat_history\" in the prompt template will be set by retrieving values from memory\n",
        "template = \"\"\"\n",
        "You are a nice chatbot having a conversation with a human.\n",
        "Previous conversation:\n",
        "{chat_history}\n",
        "New human question: {question}\n",
        "Response:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "# this is where we tell the memory that it maintains the value of chat_history\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "# and we now apply the memory in the chain\n",
        "conversation = LLMChain(llm=llm, prompt=prompt, verbose=False, memory=memory)\n",
        "\n",
        "# we just pass in question - chat_history gets populated by memory\n",
        "response = conversation({\"question\": \"hi, my name is Gizmo\"})\n",
        "print(response[\"text\"])\n",
        "\n",
        "response = conversation({\"question\": \"what was the name of the first USA president?\"})\n",
        "print(response[\"text\"])\n",
        "\n",
        "# the llm should remember us now\n",
        "response = conversation({\"question\": \"do you recall my name?\"})\n",
        "print(response[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we will get a little fancier, using a built-in ConversationChain.</br>\n",
        "We will switch to a <font color=green>ConversationSummaryMemory</font> form of memory.</br>\n",
        "This <em>summary</em> form of memory just maintains a summary of the content, to conserve memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0, max_tokens=256)\n",
        "\n",
        "summary_memory = ConversationSummaryMemory(llm=llm)\n",
        "\n",
        "conversation = ConversationChain(llm=llm, verbose=False, memory=summary_memory)\n",
        "\n",
        "answer = conversation.predict(input=\"Hi there! I am Gizmo.\")\n",
        "print(answer)\n",
        "\n",
        "answer = conversation.predict(input=\"What is the name of the first USA president?\")\n",
        "print(answer)\n",
        "\n",
        "answer = conversation.predict(input=\"Do you remember my name?\")\n",
        "print(answer)\n",
        "\n",
        "print(\"----\")\n",
        "print(conversation.memory.buffer)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMZi6NDm9npKydMJRyJPAT2",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
