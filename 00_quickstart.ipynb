{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IJvl-EEESCA"
      },
      "source": [
        "# <font color=red>LangChain:  Quickstart Guide</font>\n",
        "- https://docs.langchain.com/docs\n",
        "#### Install langchain and openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain openai  # OR, you could run the pip cmd in your shell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Put OpenAI key into your environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# do this in your shell:\n",
        "#      export OPENAI_API_KEY=\"your_key\"\n",
        "import os \n",
        "print( os.getenv(\"OPENAI_API_KEY\") )   # this will print your key\n",
        "\n",
        "# OR you can directly place it into openai\n",
        "#   but it will be readable in your code, which is not a good idea\n",
        "import openai\n",
        "openai.api_key = \"your_key\"   # key appears in clear text here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### First, let's verify that we can use LangChain with an OpenAI LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZ0GlWotE_S0"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI   # a simple completion model\n",
        "\n",
        "llm = OpenAI()\n",
        "query = \"Tell me a Dad joke about dogs.\"\n",
        "response = llm(query)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What are chains?\n",
        "A chain is a multi-step workflow, composed of links.<br/><br/>\n",
        "A link is one of:\n",
        "- a prompt\n",
        "- an LLM\n",
        "- another chain\n",
        "#### Let's do just one small chain consisting of a prompt and a chat LLM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains  import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI   # a chat model\n",
        "\n",
        "# note prompt's similarity to python f-strings\n",
        "template = \"Tell me a {joke_type} joke about {topic}.\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "# here we are only printing the prompt, not using it with an LLM\n",
        "# we just want to show that we can make substitutions for the input_variables\n",
        "print(prompt.format(joke_type=\"dad\",topic=\"dogs\"))\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.7, max_tokens=128)\n",
        "\n",
        "# now, let's create a chain consisting of the prompt and the LLM\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "response = llm_chain.run(joke_type=\"dad\",topic=\"my cat named Gizmo\")\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMZi6NDm9npKydMJRyJPAT2",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
