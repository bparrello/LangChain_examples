{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IJvl-EEESCA"
   },
   "source": [
    "# <font color=red>LangChain:  Chains</font>\n",
    "- https://docs.langchain.com/docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Does LangChain Provide?\n",
    "+ Models\n",
    "  + embedding\n",
    "  + LLM (e.g. OpenAI)\n",
    "+ Prompts\n",
    "  + prompt templates\n",
    "  + few-shot\n",
    "  + example-selectors\n",
    "  + output parsers\n",
    "<span style=\"font-family:'Comic Sans MS', cursive, sans-serif;\"><font color=orange>\n",
    "+ Chains (a multi-step workflow composed of <em>links</em>)</br>\n",
    "  + Links (one of: prompt, model, another chain)\n",
    "</font></span>\n",
    "+ Vector Database Access\n",
    "  + Document Loaders\n",
    "  + Text Splitting \n",
    "+ Memories (to facilitate chatbots or other 'iterative' sorts of apps)\n",
    "+ Agents (loop over Thought, Act, Observe)\n",
    "  + Tools\n",
    "    + math\n",
    "    + web search\n",
    "    + custom (user-defined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:'Comic Sans MS', cursive, sans-serif;\"><font color=orange>\n",
    "## Chains\n",
    "</font></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A chain is a multi-step workflow, composed of links.\n",
    "A link is one of:\n",
    "- a prompt\n",
    "- an LLM\n",
    "- another chain\n",
    "#### Our first chain will be a repeat of the one we had in the Quickstart Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a dad joke about dogs.\n",
      "Why don't we ever play hide and seek with Gizmo?\n",
      "\n",
      "Because every time, he's a purr-fect hider!\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains  import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI   # switching to a chat model\n",
    "\n",
    "# note prompt's similarity to python f-strings\n",
    "template = \"Tell me a {joke_type} joke about {topic}.\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# here we are only printing the prompt, not using it with an LLM\n",
    "# we just want to show that we can make substitutions for the input_variables\n",
    "print(prompt.format(joke_type=\"dad\",topic=\"dogs\"))\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.7, max_tokens=128)\n",
    "\n",
    "# now, let's create a chain consisting of the prompt and the LLM\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "response = llm_chain.run(joke_type=\"dad\",topic=\"my cat named Gizmo\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, a SimpleSequentialChain combining two chains, the second critiquing output of the first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mWhy don't we ever play hide and seek with Gizmo?\n",
      "\n",
      "Because whenever we do, he always paws the game!\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mThis joke is good if your audience enjoys puns and there's enough context given that Gizmo is a pet, probably a dog. The humor lies in the pun \"paws\" sounding like \"pauses,\" implying that Gizmo interrupts the game.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "This joke is good if your audience enjoys puns and there's enough context given that Gizmo is a pet, probably a dog. The humor lies in the pun \"paws\" sounding like \"pauses,\" implying that Gizmo interrupts the game.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains  import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import SimpleSequentialChain  # only pass a string between chains\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.7, max_tokens=128,)\n",
    "\n",
    "template = \"What is a good Dad joke about {topic}?\"\n",
    "joke_prompt = PromptTemplate.from_template(template)\n",
    "joke_chain = LLMChain(prompt=joke_prompt, llm=llm)\n",
    "\n",
    "template = \"Is this a good joke: {the_joke}?\"\n",
    "critic_prompt = PromptTemplate.from_template(template)\n",
    "critic_chain = LLMChain(prompt=critic_prompt, llm=llm)\n",
    "\n",
    "# chain where we run the two chains in sequence\n",
    "seq_chain = SimpleSequentialChain(chains=[joke_chain, critic_chain], verbose=True)\n",
    "critique = seq_chain.run(\"my cat named Gizmo\")\n",
    "print(critique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=green>LLMMathChain </font>demo to show an example of a specialized built-in chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMMathChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "math_chain = LLMMathChain.from_llm(llm, verbose=True)\n",
    "\n",
    "response = math_chain.run(\"What is 13 raised to the .3432 power?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will repeat our first chain here but use LangChain's new Expression Language\n",
    "The Expression Language permits you to connect links with a pipe symbol, much like in a shell.</br>\n",
    "It is not totally mature yet, and I still prefer the functional method, but we do it here for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains  import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI   # switching to a chat model\n",
    "from langchain.schema.output_parser import StrOutputParser   # output parser\n",
    "\n",
    "template = \"Tell me a {joke_type} joke about {topic}.\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "print(prompt.format(joke_type=\"dad\",topic=\"dogs\"))\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.7, max_tokens=128)\n",
    "\n",
    "llm_chain = prompt | llm | StrOutputParser()    ## <-- the new language syntax\n",
    "\n",
    "## invoke INSTEAD of run and dictionary of arguments\n",
    "response = llm_chain.invoke( {\"joke_type\": \"dad\", \"topic\": \"my cat named Gizmo\"} )\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMZi6NDm9npKydMJRyJPAT2",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
